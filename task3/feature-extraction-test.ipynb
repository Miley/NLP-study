{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'NLP']\n",
      "[('I', 'PRP'), ('love', 'VBP'), ('NLP', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "word_list = nltk.word_tokenize(\"I love NLP\")\n",
    "print(word_list)\n",
    "print(nltk.pos_tag(word_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/fv/xhx25_714xx6h8fhfqc7v45h0000gn/T/jieba.cache\n",
      "Loading model cost 0.879 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "Precise Mode: 我/ 来到/ 北京/ 清华大学\n",
      "Default Mode: 他, 来到, 了, 网易, 杭研, 大厦\n",
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "# 1.分词\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"Precise Mode: \" + \"/ \".join(seg_list))  # 精确模式\n",
    "\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\")  # 默认是精确模式\n",
    "print(\"Default Mode: \" + \", \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  # 搜索引擎模式\n",
    "print(\", \".join(seg_list))   # 此模式适合用于搜索引擎构建倒排索引的分词，粒度比较细"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'自': 2, '然': 2, '语': 2, '言': 2, '处': 2, '理': 2, '。': 2, '我': 1, '爱': 1, '是': 1, '一': 1, '个': 1, '很': 1, '有': 1, '意': 1, '思': 1, '的': 1, '研': 1, '究': 1, '领': 1, '域': 1})\n",
      "Counter({'自然语言': 2, '处理': 2, '。': 2, '我': 1, '爱': 1, '是': 1, '一个': 1, '很': 1, '有意思': 1, '的': 1, '研究': 1, '领域': 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# 字符统计\n",
    "text = \"我爱自然语言处理。自然语言处理是一个很有意思的研究领域。\"\n",
    "c = collections.Counter(text)\n",
    "print(c)\n",
    "\n",
    "# 词频统计\n",
    "import jieba\n",
    "seg_list = jieba.cut(\"我爱自然语言处理。自然语言处理是一个很有意思的研究领域。\", cut_all=False)\n",
    "c = collections.Counter(list(seg_list))\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 7)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 8)\t1\n",
      "{'他用报话机向上级呼喊 为了祖国': 2, '为了祖国 为了胜利': 0, '为了胜利 向我开炮': 1, '向我开炮 向我开炮': 3, '记者 你怎么会说出那番话': 7, '韦昌进 我只是觉得': 8, '我只是觉得 对准我自己打': 5, '对准我自己打 才有可能把上了我哨位的这些敌人打死': 4, '才有可能把上了我哨位的这些敌人打死 或者打下去': 6}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>为了祖国 为了胜利</th>\n",
       "      <th>为了胜利 向我开炮</th>\n",
       "      <th>他用报话机向上级呼喊 为了祖国</th>\n",
       "      <th>向我开炮 向我开炮</th>\n",
       "      <th>对准我自己打 才有可能把上了我哨位的这些敌人打死</th>\n",
       "      <th>我只是觉得 对准我自己打</th>\n",
       "      <th>才有可能把上了我哨位的这些敌人打死 或者打下去</th>\n",
       "      <th>记者 你怎么会说出那番话</th>\n",
       "      <th>韦昌进 我只是觉得</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   为了祖国 为了胜利  为了胜利 向我开炮  他用报话机向上级呼喊 为了祖国  向我开炮 向我开炮  对准我自己打 才有可能把上了我哨位的这些敌人打死  \\\n",
       "0          1          1                1          1                         0   \n",
       "1          0          0                0          0                         0   \n",
       "2          0          0                0          0                         1   \n",
       "\n",
       "   我只是觉得 对准我自己打  才有可能把上了我哨位的这些敌人打死 或者打下去  记者 你怎么会说出那番话  韦昌进 我只是觉得  \n",
       "0             0                        0             0          0  \n",
       "1             0                        0             1          0  \n",
       "2             1                        1             0          1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.基于sklearn的N-gram\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "data = [\"他用报话机向上级呼喊：“为了祖国，为了胜利，向我开炮！向我开炮！\",\n",
    "        \"记者：你怎么会说出那番话？\",\n",
    "        \"韦昌进：我只是觉得，对准我自己打，才有可能把上了我哨位的这些敌人打死，或者打下去。\"]\n",
    "\n",
    "# data = [\" \".join(jieba.lcut(e)) for e in data] # jieba分词，并用\" \"连接\n",
    "\n",
    "vec = CountVectorizer(min_df=1, ngram_range=(2, 2))\n",
    "# ngram_range=(1,1) 表示 unigram, ngram_range=(2,2) 表示 bigram, ngram_range=(3,3) 表示 thirgram\n",
    "# min_df 为阈值，如果某个词的文本频率小于min_df，则这个词不会被当作关键词\n",
    "\n",
    "#vec = CountVectorizer(ngram_range=(2, 4), decode_error=\"ignore\", token_pattern = r'\\b\\w+\\b',min_df=1\n",
    "#如果ngram_range=(2, 4)，则表示2，3,4个单词切割\n",
    "\n",
    "# 最后用fit()和transform()进行拟合和计算\n",
    "X = vec.fit_transform(data)  # transform text to metrix\n",
    "print(X)\n",
    "vec.get_feature_names() # get features\n",
    "\n",
    "X.toarray()\n",
    "# 查看生成的词表\n",
    "print(vec.vocabulary_)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names()) # to DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我 去', '去 云南旅游', '云南旅游 ，', '， 不仅', '不仅 去', '去 了', '了 玉龙雪山', '玉龙雪山 ，', '， 还', '还 去', '去 丽江', '丽江 古城', '古城 ，', '， 很', '很 喜欢', '喜欢 丽江', '丽江 古城']\n",
      "我 去\n",
      "去 云南旅游\n",
      "云南旅游 ，\n",
      "， 不仅\n",
      "不仅 去\n",
      "去 了\n",
      "了 玉龙雪山\n",
      "玉龙雪山 ，\n",
      "， 还\n",
      "还 去\n",
      "去 丽江\n",
      "丽江 古城\n",
      "古城 ，\n",
      "， 很\n",
      "很 喜欢\n",
      "喜欢 丽江\n",
      "丽江 古城\n"
     ]
    }
   ],
   "source": [
    "# 2.基于jieba的N-gram\n",
    "\n",
    "# 这里的_word_ngrams方法其实就是sklearn中CountVectorizer函数中用于N-Gram的方法\n",
    "def _word_ngrams(tokens, stop_words=None, ngram_range=(1,1)):\n",
    "        \"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        # handle token n-grams\n",
    "        min_n, max_n = ngram_range\n",
    "        if max_n != 1:\n",
    "            original_tokens = tokens\n",
    "            tokens = []\n",
    "            n_original_tokens = len(original_tokens)\n",
    "            for n in range(min_n, min(max_n + 1, n_original_tokens + 1)):\n",
    "                for i in range(n_original_tokens - n + 1):\n",
    "                    tokens.append(\" \".join(original_tokens[i: i + n]))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "text = \"我去云南旅游，不仅去了玉龙雪山，还去丽江古城，很喜欢丽江古城\"\n",
    "import jieba\n",
    "cut = jieba.cut(text)\n",
    "listcut = list(cut)\n",
    "n_gramWords = _word_ngrams(tokens = listcut, ngram_range=(2,2))\n",
    "print(n_gramWords)\n",
    "for n_gramWord in n_gramWords:\n",
    "    print(n_gramWord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.基于NLTK的N-gram\n",
    "import nltk\n",
    "\n",
    "list(nltk.bigrams(['a','b','c']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 8)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 8)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 6)\t2\n",
      "  (2, 3)\t1\n",
      "  (2, 8)\t1\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 1 1 0 1]\n",
      " [1 1 1 1 1 0 2 1 1]]\n",
      "  (0, 8)\t0.42040098658605557\n",
      "  (0, 6)\t0.42040098658605557\n",
      "  (0, 3)\t0.42040098658605557\n",
      "  (0, 2)\t0.5413428136679054\n",
      "  (0, 1)\t0.42040098658605557\n",
      "  (1, 8)\t0.3816141458138271\n",
      "  (1, 6)\t0.3816141458138271\n",
      "  (1, 5)\t0.6461289150464732\n",
      "  (1, 3)\t0.3816141458138271\n",
      "  (1, 1)\t0.3816141458138271\n",
      "  (2, 8)\t0.2407133333247863\n",
      "  (2, 7)\t0.4075631016420483\n",
      "  (2, 6)\t0.4814266666495726\n",
      "  (2, 4)\t0.4075631016420483\n",
      "  (2, 3)\t0.2407133333247863\n",
      "  (2, 2)\t0.30996224392243715\n",
      "  (2, 1)\t0.2407133333247863\n",
      "  (2, 0)\t0.4075631016420483\n",
      "  (0, 8)\t0.42040098658605557\n",
      "  (0, 3)\t0.42040098658605557\n",
      "  (0, 6)\t0.42040098658605557\n",
      "  (0, 2)\t0.5413428136679054\n",
      "  (0, 1)\t0.42040098658605557\n",
      "  (1, 8)\t0.3816141458138271\n",
      "  (1, 3)\t0.3816141458138271\n",
      "  (1, 6)\t0.3816141458138271\n",
      "  (1, 1)\t0.3816141458138271\n",
      "  (1, 5)\t0.6461289150464732\n",
      "  (2, 8)\t0.2407133333247863\n",
      "  (2, 3)\t0.2407133333247863\n",
      "  (2, 6)\t0.4814266666495726\n",
      "  (2, 2)\t0.30996224392243715\n",
      "  (2, 1)\t0.2407133333247863\n",
      "  (2, 0)\t0.4075631016420483\n",
      "  (2, 7)\t0.4075631016420483\n",
      "  (2, 4)\t0.4075631016420483\n"
     ]
    }
   ],
   "source": [
    "## 文本矩阵化\n",
    "\n",
    "#CountVectorizer 类会将文本中的词语转换为词频矩阵，例如矩阵中包含一个元素a[i][j]，它表示j词在i类文本下的词频。它通过 fit_transform 函数计算各个词语出现的次数，通过get_feature_names()可获取词袋中所有文本的关键字，通过 toarray()可看到词频矩阵的结果。\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "corpus = ['This is the first document.', 'This is the second document.', 'And the third one.' 'Is this the first document?']\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "\n",
    "print(X)   #左边的括号中的第一个数字是文本的序号i，第2个数字是词的序号j，注意词的序号是基于所有的文档的。第三个数字就是我们的词频。\n",
    "print(feature_name)\n",
    "print(X.toarray())\n",
    "# 由于大部分文本都只会用词汇表中很少一部分的词，因此词向量中有大量的0，也就是说词向量是稀疏的。因此在实际应用中一般使用稀疏矩阵来存储。\n",
    "\n",
    "\n",
    "\n",
    "# 因此，有些词在文本中尽管词频高，但是并不重要，这个时候就可以用TF-IDF技术\n",
    "# 1. CountVectorizer 结合 TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))\n",
    "print(tfidf)\n",
    "\n",
    "# 2.用 TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf2 = TfidfVectorizer()\n",
    "re = tfidf2.fit_transform(corpus)\n",
    "print(re)\n",
    "\n",
    "\n",
    "\n",
    "## jieba的TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
